{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Path to your OpenAI API key\n",
    "api_key_location=os.path.expanduser(os.path.join(\"~/.ssh/\", \"openai\")) \n",
    "\n",
    "# Setting up the API key\n",
    "with open(api_key_location) as f:\n",
    "    openai.api_key = f.read().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_chat_prompt will take as input a tuple of indices corresponding to concepts in `labels`\n",
    "def generate_prompt(current_pair):\n",
    "    prompt = f\"\"\"How related are the two concepts on a scale of 0 (unrelated) to 1 (highly related)? Reply with a numerical rating and no other text.\"\"\"\n",
    "    prompt += f'''\\nConcept 1: {labels[current_pair[0]]}\\nConcept 2: {labels[current_pair[1]]}\\nRating:'''\n",
    "    return prompt\n",
    "\n",
    "# Let's try an example\n",
    "current_pair=(0,1)\n",
    "prompt=generate_prompt(current_pair)\n",
    "print(prompt)\n",
    "messages=[{\"role\":\"user\", \"content\":prompt}]\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 1 # Higher temperature makes GPT more creative\n",
    "model = \"gpt-3.5-turbo\"\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect GPT Responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openai_chat_completion(messages, model, temperature):\n",
    "    response=False\n",
    "    i=0\n",
    "    while not response:\n",
    "        i+=1\n",
    "        try:\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    temperature=temperature,\n",
    "                )\n",
    "        except Exception as e:\n",
    "            if i>=5: #if error persis after 5 attempts, give up\n",
    "                return False, False\n",
    "            if i>=4:\n",
    "                print(f'Attempt {i} failed: {e}')\n",
    "            elif i>=3: print(f'Attempt {i} failed.')\n",
    "            time.sleep(5) # wait before pinging the API again in case error is due to rate limit\n",
    "    choices = [dict(choice.items()) for choice in response.choices]\n",
    "    return choices, response.created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_similarities = np.zeros((len(labels),len(labels)))\n",
    "visited_pairs=[]\n",
    "\n",
    "for idx1, bname1 in enumerate(tqdm(labels)):\n",
    "    for idx2, bname2 in enumerate(tqdm(labels,desc=f'Generating predictions {idx1}/{len(labels)}')):\n",
    "        \n",
    "        current_pair = (idx1, idx2)\n",
    "        \n",
    "        #We assume that the similarity matrix is symmetric which means we can skip entries from the lower triangle\n",
    "        if current_pair in visited_pairs:\n",
    "            # print(f'Already visited pair {current_pair}')\n",
    "            continue\n",
    "            \n",
    "            \n",
    "        prompt = generate_prompt(current_pair)\n",
    "        response=False\n",
    "        answer=False\n",
    "        attempt=0\n",
    "        while not answer:\n",
    "            attempt+=1\n",
    "            messages=[{'role':'user', 'content':prompt}]\n",
    "            choices,created=openai_chat_completion(messages, model, temperature)\n",
    "            try:\n",
    "                answer = choices[0]['message']['content'].strip().split()[0]\n",
    "                predicted_similarities[idx1, idx2] = float(answer)\n",
    "                predicted_similarities[idx2, idx1] = float(answer) # Because we assume the matrix is symmetric\n",
    "                if idx1 == idx2:\n",
    "                    visited_pairs.append((idx1, idx2))\n",
    "                else:\n",
    "                    visited_pairs.append((idx1, idx2))\n",
    "                    visited_pairs.append((idx2, idx1))\n",
    "            #In case we fail to parse out an answer 10 times in a row, we skip that entry\n",
    "            except Exception as e: \n",
    "                print(answer)\n",
    "                print(e)\n",
    "                answer=False\n",
    "                if attempt>10:\n",
    "                    print('Error', cache[key], e)\n",
    "\n",
    "\n",
    "#Create a directory where we will save the model predictions\n",
    "os.makedirs(f'RC/predictions/RC-fruits', exist_ok=True)\n",
    "\n",
    "# Save the model predictions\n",
    "np.save(f'RC/predictions/RC-fruits/RC-fruits-{model}-{temperature}-{seed}.npy', predicted_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taylory-E5piFvX3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
